import{b as j,e as ee,a as v,t as _,c as T}from"../chunks/disclose-version.RMkl_-ui.js";import{i as V}from"../chunks/legacy.CgMNwOP2.js";import{L as te,v as U,h as F,q as se,f as D,a as ae,k as ne,b as re,e as oe,g as ie,an as le,s as a,c as d,r as i,t as S,A as b,C as c,u as q,x as z,aF as M,D as $,B as k}from"../chunks/runtime.CBu_r9p9.js";import{e as ce}from"../chunks/store.CqJFsyPF.js";import{s as O}from"../chunks/render.yGww_O-B.js";import{e as W,s as p,I as A,D as de,b as he,c as me,i as G,A as fe,h as pe,a as P,d as ue,f as ge}from"../chunks/Icon.CR9FrAyB.js";import{b as N}from"../chunks/this.BZAQLy_4.js";import{p as ve}from"../chunks/props.-FiUUMrw.js";import{o as K}from"../chunks/index-client.5ZHUkz05.js";import{M as xe}from"../chunks/MultipleChoices.D8S4aGJW.js";import{V as B}from"../chunks/versions.Cg8Xt6PH.js";function be(m,n,s,e,t){var o=m,l="",h;te(()=>{if(l===(l=n()??"")){U&&F();return}h!==void 0&&(le(h),h=void 0),l!==""&&(h=se(()=>{if(U){D.data;for(var r=F(),g=r;r!==null&&(r.nodeType!==8||r.data!=="");)g=r,r=ae(r);if(r===null)throw ne(),re;j(D,g),o=oe(r);return}var f=l+"",u=ee(f);j(ie(u),u.lastChild),o.before(u)}))})}function _e(m,n,s,e,t){var h;U&&F();var o=(h=n.$$slots)==null?void 0:h[s],l=!1;o===!0&&(o=n.children,l=!0),o===void 0||o(m,l?()=>e:e)}var we=_('<a class="lexend text-base text-[#5A9485]"> </a>'),Ce=_('<section class="relative flex h-svh flex-col items-center justify-center border-b-2 border-t-2 border-[#212121] bg-[#0E0E0E] px-6"><h1 class="montserrat text-4xl font-extrabold text-white">MC-LARC</h1> <h2 class="lexend mt-1 text-lg font-medium text-white">From Generation to Selection</h2> <div class="mt-6 flex flex-col items-center gap-4"></div> <p class="lexend absolute bottom-8 mt-2 text-sm text-gray-600">Gwangju Institute of Science and Technology</p> <div class="absolute bottom-16 flex gap-2"><a target="_blank" rel="noopener noreferrer" class="lexend flex items-center gap-2 rounded-lg px-4 py-2 text-base font-medium text-white hover:bg-gray-800"><!> Paper</a> <a target="_blank" rel="noopener noreferrer" class="lexend flex items-center gap-2 rounded-lg px-4 py-2 text-base font-medium text-white hover:bg-gray-800"><!> Code</a> <a target="_blank" rel="noopener noreferrer" class="lexend flex items-center gap-2 rounded-lg px-4 py-2 text-base font-medium text-white hover:bg-gray-800"><!> Dataset</a></div></section>');function ye(m){let n=[{name:"Donghyeon Shin*",link:"https://onemain1.github.io/"},{name:"Seungpil Lee*",link:"https://iamseungpil.github.io/"},{name:"Klea Lena Kovačec",link:""},{name:"Sundong Kim†",link:"https://sundong.kim/"}];const s="https://aclanthology.org/2024.findings-emnlp.392/",e="https://github.com/GIST-DSLab/MC-LARC",t="/download";var o=Ce(),l=a(d(o),4);W(l,5,()=>n,G,(w,y)=>{var C=we(),L=d(C,!0);i(C),S(()=>{p(C,"href",c(y).link),O(L,c(y).name)}),v(w,C)}),i(l);var h=a(l,4),r=d(h);p(r,"href",s);var g=d(r);A(g,{src:de,class:"h-4 w-4"}),b(),i(r);var f=a(r,2);p(f,"href",e);var u=d(f);A(u,{src:he,class:"h-4 w-4"}),b(),i(f);var x=a(f,2);p(x,"href",t);var R=d(x);A(R,{src:me,class:"h-4 w-4"}),b(),i(x),i(h),i(o),v(m,o)}var Ae=_('<div class="sticky top-0 z-10 w-full px-8 py-8"><div class="w-full border-2 border-white bg-black bg-opacity-30 px-5 py-3 backdrop-blur-md"><h1 class="lexend text-lg font-semibold text-white"><!></h1></div></div>');function E(m,n){var s=Ae(),e=d(s),t=d(e),o=d(t);_e(o,n,"default",{}),i(t),i(e),i(s),v(m,s)}var Re=_(`<section class="flex flex-col items-center justify-center border-b-2 border-t-2 border-[#212121] bg-[#0E0E0E]"><!> <p class="lexend px-8 pb-8 text-justify text-base font-light leading-7 text-white">As artificial intelligence reasoning abilities gain prominence, generating reliable benchmarks
		becomes crucial. The Abstract and Reasoning Corpus (ARC) offers challenging problems yet
		unsolved by AI. While ARC effectively assesses reasoning, its generation-based evaluation
		overlooks other assessment aspects. Bloom's Taxonomy suggests evaluating six cognitive stages:
		Remember, Understand, Apply, Analyze, Evaluate, and Create. To extend ARC's focus beyond the
		Create stage, we developed MC-LARC, a multiple-choice format suitable for assessing stages like
		Understand and Apply in Large Language Models (LLMs). Our evaluation of ChatGPT4V's analogical
		reasoning using MC-LARC confirmed that this format supports LLMs' reasoning capabilities and
		facilitates evidence analysis. However, we observed LLMs using shortcuts in MC-LARC tasks. To
		address this, we propose a self-feedback framework where LLMs identify issues and generate
		improved options.</p></section>`);function Le(m){var n=Re(),s=d(n);E(s,{children:(e,t)=>{b();var o=T("4. Abstract");v(e,o)},$$slots:{default:!0}}),b(2),i(n),v(m,n)}var ke=_('<div class="relative flex h-[600vh] w-full justify-center bg-black"><div class="img-max h-[100vh] w-full"><video class="h-full w-full object-contain" preload="auto" playsinline=""></video></div></div>',2);function H(m,n){q(n,!1);let s=ve(n,"videoSrc",8),e=$(),t=$();K(()=>{const r=new IntersectionObserver(f=>{f.forEach(u=>{u.isIntersecting&&(M(e,c(e).style.position="sticky"),M(e,c(e).style.top="0"))})},{threshold:0});c(e)&&r.observe(c(e)),c(t)&&(c(t).load(),M(t,c(t).currentTime=0),c(t).addEventListener("loadedmetadata",()=>{window.addEventListener("scroll",g)}));const g=()=>{const f=c(e).getBoundingClientRect(),u=c(e).parentElement.getBoundingClientRect(),x=1-Math.max(0,Math.min(1,(u.bottom-f.bottom)/(u.height-f.height)));c(t)&&c(t).duration&&!isNaN(c(t).duration)&&requestAnimationFrame(()=>{M(t,c(t).currentTime=c(t).duration*x)})};return()=>{window.removeEventListener("scroll",g),r.disconnect()}}),V();var o=ke(),l=d(o),h=d(l);h.muted=!0,N(h,r=>k(t,r),()=>c(t)),i(l),N(l,r=>k(e,r),()=>c(e)),i(o),S(()=>p(h,"src",s())),v(m,o),z()}const Me=""+new URL("../assets/Ch1_Conventional_Arc_1.BHU3nWqi.png",import.meta.url).href,Se=""+new URL("../assets/Ch1_Conventional_Arc_2.Dd3man6P.png",import.meta.url).href,$e=""+new URL("../assets/1_before.DWWhPRP-.png",import.meta.url).href,Te=""+new URL("../assets/1_after.MonmQB-F.png",import.meta.url).href,Ee=""+new URL("../assets/2_before.BTE2VZrQ.png",import.meta.url).href,Ie=""+new URL("../assets/2_after.Ce7_sX5U.png",import.meta.url).href,Pe=""+new URL("../assets/3_before.bDOlpUJB.png",import.meta.url).href,Ue=""+new URL("../assets/3_after.CqtVJNwI.png",import.meta.url).href,Fe=""+new URL("../assets/Ch1_MC_LARC_EX.Z0TNMD_k.mp4",import.meta.url).href;var Qe=_('<a class="shrink-0"><img class="w-40 rounded-lg" loading="lazy"> <div class="lexend mt-2 text-center text-sm text-neutral-400"> </div></a>'),je=_('<section class="flex w-full flex-col items-center justify-center px-4 py-20"><h3 class="lexend text-2xl font-bold text-white">Visualizations</h3> <p class="lexend mt-1 w-full text-center text-base font-light text-neutral-400">You can browse whole MC-LARC dataset here.</p> <div class="relative mt-8 w-full overflow-hidden"><div class="flex gap-4" role="region"></div></div> <a class="lexend mt-10 flex items-center gap-2 rounded-3xl bg-white px-5 py-2 text-lg">See All Visualizations <!></a></section>');function De(m,n){q(n,!1);let s=$([]),e=[],t=$(0);function o(){return e[Math.floor(Math.random()*e.length)]}function l(){k(t,c(t)-1),c(t)<=-176&&(k(t,0),k(s,[...c(s).slice(1),o()])),requestAnimationFrame(l)}K(async()=>{e=await(await fetch("/assets/choices/output.json")).json(),k(s,Array.from({length:5},o)),requestAnimationFrame(l)}),V();var h=je(),r=a(d(h),4),g=d(r);W(g,5,()=>c(s),G,(x,R)=>{var w=Qe(),y=d(w),C=a(y,2),L=d(C,!0);i(C),i(w),S(()=>{p(w,"href",`/task/${B.Original.name}/${c(R)}`),p(y,"src",`/assets/thumbnails/${c(R)}.png`),p(y,"alt",c(R)),O(L,c(R))}),pe(y),v(x,w)}),i(g),i(r);var f=a(r,2),u=a(d(f));A(u,{src:fe,class:"h-6 w-6",mini:!0}),i(f),i(h),S(()=>{p(g,"style",`transform: translateX(${c(t)??""}px);`),p(f,"href",`/task/${B.Original.name}`)}),v(m,h),z()}var Ne=_(`<section class="flex flex-col items-center justify-center"><!> <h3 class="lexend text-xl font-bold text-white">MC-LARC</h3> <p class="lexend px-8 py-4 text-base font-light leading-7 text-white">1. Let's find the rules for transforming left to right. <br> 2. Solve this multichoice questions.</p> <div class="img-max grid w-full grid-cols-[1fr_auto_1fr] place-items-center text-white"><img class="w-full" alt="Ch1_MC_LARC_Q_1_before"> <!> <img class="w-full" alt="Ch1_MC_LARC_Q_1_after"> <img class="w-full" alt="Ch1_MC_LARC_Q_2_before"> <!> <img class="w-full" alt="Ch1_MC_LARC_Q_2_after"> <img class="w-full" alt="Ch1_MC_LARC_Q_3_before"> <!> <img class="w-full" alt="Ch1_MC_LARC_Q_3_after"></div> <div class="mt-4 w-full px-8"><!></div> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">See? This is <b class="font-semibold">exactly as same as how MC-LARC works</b>. <br> The wrong answers are generated with ChatGPT4.</p> <!> <h3 class="lexend mb-2 mt-14 text-xl font-bold text-white">Original ARC</h3> <img class="img-max w-full px-3" alt="Ch1_Conventional_Arc_1"> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">ARC (Abstraction and Reasoning Corpus) is a benchmark designed to test AI's abstract reasoning
		capabilities. Each task consists of input-output pairs where AI must identify patterns and apply
		them to new inputs.</p> <img class="img-max mt-4 w-full px-3" alt="Ch1_Conventional_Arc_2"> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">However, ARC is highly sensitive to even minor mistakes. A single pixel error in pattern
		recognition can lead to complete task failure, making it challenging to achieve consistent
		performance.</p> <div class="lexend mt-8 flex w-full items-center justify-center gap-2 text-center text-xl italic text-gray-400"><!> Scroll to play</div> <!></section>`);function Be(m){const n=["Fill the empty spaces within the yellow pattern with green pixels.","Fill the empty spaces outside the green pattern with yellow pixels.","Fill the empty spaces within each green pattern with yellow pixels."];var s=Ne(),e=d(s);E(e,{children:(Y,Ze)=>{b();var Z=T("1. ARC vs MC-LARC");v(Y,Z)},$$slots:{default:!0}});var t=a(e,6),o=d(t);p(o,"src",$e);var l=a(o,2);A(l,{src:P,class:"h-6 w-6",solid:!0,micro:!0});var h=a(l,2);p(h,"src",Te);var r=a(h,2);p(r,"src",Ee);var g=a(r,2);A(g,{src:P,class:"h-6 w-6",solid:!0,micro:!0});var f=a(g,2);p(f,"src",Ie);var u=a(f,2);p(u,"src",Pe);var x=a(u,2);A(x,{src:P,class:"h-6 w-6",solid:!0,micro:!0});var R=a(x,2);p(R,"src",Ue),i(t);var w=a(t,2),y=d(w);xe(y,{choices:n,answer:3}),i(w);var C=a(w,4);De(C,{});var L=a(C,4);p(L,"src",Me);var Q=a(L,4);p(Q,"src",Se);var I=a(Q,4),X=d(I);A(X,{src:ue,class:"h-6 w-6",solid:!0,micro:!0}),b(),i(I);var J=a(I,2);H(J,{videoSrc:Fe}),i(s),v(m,s)}const Ve=""+new URL("../assets/Ch2_MC_LARC_Without_Image.DcvIXmJI.png",import.meta.url).href,qe=""+new URL("../assets/Ch2_Precision.sPUvWM4m.png",import.meta.url).href,ze=""+new URL("../assets/Ch2_Pattern.DJeqVNfB.mp4",import.meta.url).href;var Oe=_(`<section class="flex flex-col items-center justify-center border-b-2 border-t-2 border-[#212121] bg-[#0E0E0E] pb-4"><!> <img class="img-max w-full px-3" alt="Ch2_MC_LARC_Without_Image"> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">Imagine solving the previous question <u>without seeing the image.</u> You'll probably think it's nonsense, since the choices are about the images.</p> <img class="img-max w-full px-3" alt="Ch2_Precision"> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">However, the nonsense occured while testing MC-LARC with the image removed. The benchmark should
		approximate 20% precision, <strong class="font-semibold">but the practical result showed 65%.</strong></p> <div class="pointer-events-none inset-0 h-40 w-full bg-gradient-to-b from-transparent to-[#000000]"></div> <!> <div class="pointer-events-none inset-0 h-40 w-full bg-gradient-to-t from-transparent to-[#000000]"></div> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">Models could solve the problem without the image by <u>finding word patterns</u> or based on sentence
		length.</p></section>`);function We(m){var n=Oe(),s=d(n);E(s,{children:(l,h)=>{b();var r=T("2. Shortcut Problem");v(l,r)},$$slots:{default:!0}});var e=a(s,2);p(e,"src",Ve);var t=a(e,4);p(t,"src",qe);var o=a(t,6);H(o,{videoSrc:ze}),b(4),i(n),v(m,n)}const Ge=""+new URL("../assets/Ch3_Chat.C5Q2Gp0g.png",import.meta.url).href;var Ke=_(`<section class="flex flex-col items-center justify-center pb-4"><!> <p class="lexend mb-4 w-full px-8 text-base font-light leading-7 text-white">To prevent the model from deducing the answer from the choices, we introduced the <strong class="font-semibold">Self-Feedback Framework</strong>.</p> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">First, we provide the model with choices <u>without images</u>.</p> <img class="img-max w-full px-3 py-4" alt="Ch3_Chat"> <p class="lexend w-full px-8 py-4 text-base font-light leading-7 text-white">And then instruct the model to reconstruct the choices in a direction that prevents it from
		deducing the answer from the choices.</p></section>`);function He(m){var n=Ke(),s=d(n);E(s,{children:(t,o)=>{b();var l=T("3. Self-feedback Framework");v(t,l)},$$slots:{default:!0}});var e=a(s,6);p(e,"src",Ge),b(2),i(n),v(m,n)}var Xe=_(`<footer class="flex flex-col items-center justify-center px-8 py-12"><h3 class="lexend w-full text-left text-3xl font-bold leading-7 text-white">Bibtex</h3> <code class="mt-4 w-full rounded-lg bg-neutral-800 px-6 py-4 text-left text-sm font-light leading-7 text-white"><!></code> <h2 class="montserrat mt-16 w-full text-left text-4xl font-bold leading-7 text-neutral-400">MC-LARC</h2> <p class="lexend mt-4 w-full text-left text-sm font-light leading-7 text-neutral-400">page made by <a href="https://github.com/crown-3" class="underline">crown3</a></p> <p class="lexend mt-4 w-full text-left text-sm font-light leading-7 text-neutral-400">This work was supported by the IITP (RS-2023-00216011, RS-2024-00445087, No. 2019-0-01842) and
		the NRF (RS-2024-00451162) grants funded by the Ministry of Science and ICT, Korea. Experiments
		were supported by the Accelerate Foundation Models Research Initiative, Microsoft.</p></footer>`);function Je(m){var n=Xe(),s=a(d(n),2),e=d(s);be(e,()=>`@inproceedings{shin2024from,<br>
		&nbsp;&nbsp;&nbsp;&nbsp;title={From Generation to Selection Findings of Converting Analogical Problem-Solving into Multiple-Choice Questions},<br>
		&nbsp;&nbsp;&nbsp;&nbsp;booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},<br>
		&nbsp;&nbsp;&nbsp;&nbsp;author={Shin, Donghyeon and Lee, Seungpil and Kovacec, Klea and Kim, Sundong},<br>
		&nbsp;&nbsp;&nbsp;&nbsp;year={2024},<br>
		}`),i(s),b(6),i(n),v(m,n)}var Ye=_('<div class="flex flex-col items-center justify-center"><div class="container max-w-screen-sm bg-black"><!> <!> <!> <!> <!> <!></div> <button class="fixed bottom-4 right-4 z-20 mt-4 rounded-full border-[1px] border-neutral-600 bg-neutral-600 bg-opacity-30 px-2 py-2 backdrop-blur-md"><!></button></div>');function ht(m){const n=()=>{window.scrollTo({top:0,behavior:"smooth"})};var s=Ye(),e=d(s),t=d(e);ye(t);var o=a(t,2);Be(o);var l=a(o,2);We(l);var h=a(l,2);He(h);var r=a(h,2);Le(r);var g=a(r,2);Je(g),i(e);var f=a(e,2),u=d(f);A(u,{src:ge,class:"h-6 w-6 text-white",micro:!0}),i(f),i(s),ce("click",f,n),v(m,s)}export{ht as component};
